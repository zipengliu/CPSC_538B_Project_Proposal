\section{Distributed State}
\label{sec:distributed-state}


Distributed systems have complex state. Each node communicating in a network
relies upon its state to enact a protocol, or perform distributed computation.
Reasoning about distributed state is difficult due to asynchronous execution. In
order to develop correct systems, state must be understood. Different aspects of
distributed state contain important information for understanding a systems
behaviour. In particular a complete set of variable values for all nodes in a
system is useful for debugging. Inherent to distributed systems is their
concurrent execution, which leads to a fundamental lack of clock
synchronization. Without synchronization, determining the precise values of a
systems variables at a moment in time is impossible. Distributed snaphosts
provide an approximate view of a distributed system, which can be used to reason
about and understand distributed state. For instance snapshots are useful for
determining global predicates, and inferring the invariants of a systems
execution. Unfortunately interpreting snapshots is difficult, they are composed
of partially ordered events, and can extend over large periods of real time.

We propose a set of techniques for extracting important information from
distributed snapshots, and aggregating information from them, into meaningful
visualizations. First, we apply a novel differentiation function to distributed
snapshots which constructs an implicit finite state machine from an execution.
Second we leverage know techniques for build a communication graph, and runtime
invariants. For each of these we encode them into meaningful visualization
which can help developer understand their systems, and detect deviant
behaviour.

\subsection{State Differentiation}

Distributed state is inherently more complicated than the state of an
individual process. However, due to the conventions of typical system designs,
state can behave predictably. For example, most distributed systems are built
using protocol specifications which define a finite state machines for the
systems behaviour. The state of instantiated systems follow the logic of their
protocols, and the state of the system is altered accordingly. Our primary
contribution of this work is a state differentiation technique which extracts
protocol specific behavior from the logs of a distributed system.

\subsubsection{System Instrumentation}

In order to extract the state of a distributed system it's variables must be
logged. We use Dinv, a tool for detecting likely data invariants in distributed
systems as an instrumentation tool. Dinv instruments systems by generating
logging code to the entrance and exit of each function in a program. Dinv uses
program slicing to determining which variables affect, or are affected by
messages passed over a network. Each logging function contains the set of all
in scope network affected variables. Such variables are more likely to contain
state pertinent to a protocols implementation. At runtime the values of
variables in the logging statements are written to disk, along with vector
clocks, which Dinv martians in the background.

\subsubsection{Distributed Snapshot Collection}

Collecting snapshots is done on the logs generated by executing an instrumented
system. We once again leverage Dinv to compute distributed snapshots from these
logs. The complete set of distributed snapshots is a partially ordered
collection of system states. Each snapshot contains the state of each node in
the system, and the values of their variables at the time the snapshot was
taken.

\subsubsection{Differentiation Algorithm}

To create a finite state machine from snapshots alone requires that the
snapshots must be compared with one another to determine which part of the FSM
they correspond to. Snapshots contain all network interacting variables; As
such no single function can be used to compare variables which respects the
semantics of their types. In order to compare the states of all variables
uniformly we encode them into their binary reforestation. We define $v_{xor}$ to
be the result of calculating the xor of two variables. Each snapshot contains
variable instances, the first step in our differentiation algorithm is to
compute $v_{xor}$ on each variable, with its occurrence in each snapshot.


Formally a node $n$ in a distributed system is composed of variables $V \in
v_1,v_2\dots v_n$. A distributed system is composed of nodes $N \in n_1, n_2,
\dots , n_m$. A snapshot $s$ is a $n x m$ matrix

$s_{m,n} = $
$\begin{pmatrix}
  v_{1,1} & v_{1,2} & \cdots & v_{1,n} \\
  v_{2,1} & v_{2,2} & \cdots & v_{2,n} \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  v_{m,1} & v_{m,2} & \cdots & v_{m,n} 
 \end{pmatrix}$

Finally $C$ is a partially ordered collection of snapshots $s_1, s_2, \dots
s_k$ from a systems execution. Computing xor on two snapshots $i,j$ results in
the matrix 

$s^{xor}_{i,j} = $
$\begin{pmatrix}
  xor(v_{i1,1},v_{j1,1}}) & xor(v_{i1,2},v_{j1,2}}) & \cdots & xor(v_{i1,n},v_{j1,n}}) \\
  xor(v_{i2,1},v_{j2,1}}) & xor(v_{i2,2},v_{j2,2}}) & \cdots & xor(v_{i2,n},v_{j2,n}}) \\
  \vdots  & \vdots  & \ddots & \vdots  \\
  xor(v_{im,1},v_{jm,1}}) & xor(v_{im,2},v_{jm,2}}) & \cdots & xor(v_{im,n},v_{jm,n}}) \\
 \end{pmatrix}$

We construct a $k$ x $k$ plane $P^{xor}$ where $\forall i,j \in C, P^{xor}_{i,j} =
s^{xor}_{i,j}$.

Each $v_{xor}$ is a binary string where each 1 bit corresponds to a difference
in bit correspondence between two variable instances. To construct a difference
measure between the two variables we compute the hamming weight of each
$v_{xor}$ by counting it's one bits, denoted $v_{hw}$ which reduces each
variable to a single integer. The next step in the algorithm is to compute
$v_{hm} \forall v_{xor} \in P$ denoted $P^{hm}$. Each index in $P^{hm}$ is a
vector of integers. The final step in our state differentiation algorithm is to
reduce each vector in $P^{hm}$ to a single value representing the difference in
state between two snapshots. This is done by computing the euclidean norm of
each vector. Formally we compute $P^n$ where $\forall i,j \in P^{hm}, P^n_{i,j}
= \sqrt{(P^{hm}_{i,j,1})^2 + (P^{hm}_{i,j,2})^2 +\dots + (P^{hm}_{i,j,n})^2 }$.

The indices of $P^n$ are distances between the states of snapshots.
Section~\ref{time curve} explains the procedure by which these distances are
plotted graphically, and linked temporally.

\subsection{Distributed Invariants}

State machines are useful for understanding transitions in a systems behaviour.
However, they only represent differences in state and do not reveal any of the
states underlying properties. Invariants are properties that hold at all times.
System invariants are typically defined in a system, or protocol specification.
Understanding the invariant properties of a system is key for reasoning about
it's behaviour. In addition to leveraging Dinv as a snapshot collection tool,
we also use it's likely data invariant detection facilities to gain richer information
about an execution.

Dinv detects invariants by analyzing the logs of a systems execution. It uses
dynamic analysis techniques to reason about concurrency, and tests the logs for
data invariants. These invariants span multiple nodes and provide a high level
overview of a systems execution. Dinv does not check specified invariants;
instead it tests states on a list of template  invariants and outputs all
invariants which were not violated. The number of invariants detected can be
large. Further, many invariants are transitive, and interpreting their
significance requires expertise. We produce a graph of Dinv's invariant output
to consolidate large amounts of data and help users understand transitive
invariants. Section ~\ref{invariant visualization} details our invariant
graphing technique.

\subsubsection{Communication Graph}

One of the biggest challenges in understating distributed systems is
concurrency. Messages are passed between nodes asynchronously, and their delivery
is non deterministic. Developers have to design their systems operate correctly
under these conditions. Communication is a central component of distributed
systems. Understanding how nodes in a system communicate is key for reasoning
about corner cases, and checking that a system has executed correctly. A
canonical way of visualizing message passing is a communication
graph~\cite{lamport}{ShiViz}. We compute communication graphs from the logs of
an execution. Logs of a systems execution also contain vector clocks. Using the
vector clocks from the logs we use the happens-before relationship to construct
a communication graph. Section~\ref{communication graph} covers the specifics
of how the graph is rendered.

















